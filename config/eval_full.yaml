# data config
dataset: pixel-coping/pubmed_derived
split: nonbiomedical
max_token_length: 1024
subset_examples: null
tokenizer: meta-llama/Llama-2-7b-hf

# model config
model: save/full

# trainer config
batch_size: 8
accumulate_grad_batches: 1
max_epochs: 1
lr: 3e-6
warmup_ratio: 0.1
scheduler: linear
gradient_clip_val: 1.0

# global config
mode: eval
seed: 1